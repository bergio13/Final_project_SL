{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/FreTS-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia5dCvpSD-c4",
        "outputId": "5e619f20-60c4-465c-e9fe-6bde8eb53579"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/FreTS-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2ApiIHUZDqFF"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from models import FreTS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "OskTOZQoE4JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Dataset_ETT_hour(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            mms.fit(train_data.values)\n",
        "            data = mms.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        return mms.fit_transform(data.cpu())\n",
        "\n",
        "\n",
        "class Dataset_ETT_minute(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTm1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='t'):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "class Dataset_Covid(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.train_only = train_only\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        df_raw = df_raw.dropna()\n",
        "\n",
        "        cols = list(df_raw.columns)\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        cols.remove('date')\n",
        "\n",
        "        num_train = int(len(df_raw) * (0.6 if not self.train_only else 1))\n",
        "        num_test = int(len(df_raw) * 0.2)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        ## min max scaler\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            mms.fit(train_data.values)\n",
        "            data = mms.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        return mms.fit_transform(data.cpu())\n",
        "\n",
        "#min max scaler\n",
        "class Dataset_Custom_(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.train_only = train_only\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        df_raw = df_raw.dropna()\n",
        "\n",
        "        cols = list(df_raw.columns)\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        cols.remove('date')\n",
        "\n",
        "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
        "        num_test = int(len(df_raw) * 0.1)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        ## min max scaler\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            mms.fit(train_data.values)\n",
        "            data = mms.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        mms = MinMaxScaler(feature_range=(0, 1))\n",
        "        return mms.fit_transform(data.cpu())\n",
        "        #return self.scaler.inverse_transform(data)\n",
        "\n",
        "class Dataset_Custom(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=False, timeenc=0, freq='h', train_only=False):\n",
        "\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.train_only = train_only\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        df_raw = df_raw.dropna()\n",
        "\n",
        "        cols = list(df_raw.columns)\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        cols.remove('date')\n",
        "\n",
        "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
        "        num_test = int(len(df_raw) * 0.1)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "class Dataset_Pred(Dataset):\n",
        "    def __init__(self, root_path, flag='pred', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None, train_only=False):\n",
        "\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['pred']\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols = cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        if self.cols:\n",
        "            cols = self.cols.copy()\n",
        "        else:\n",
        "            cols = list(df_raw.columns)\n",
        "            self.cols = cols.copy()\n",
        "            cols.remove('date')\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        border1 = len(df_raw) - self.seq_len\n",
        "        border2 = len(df_raw)\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            self.scaler.fit(df_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        tmp_stamp = df_raw[['date']][border1:border2]\n",
        "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
        "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)\n",
        "\n",
        "        df_stamp = pd.DataFrame(columns=['date'])\n",
        "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
        "        self.future_dates = list(pred_dates[1:])\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = self.data_x[r_begin:r_begin + self.label_len]\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_begin + self.label_len]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n"
      ],
      "metadata": {
        "id": "dZQrLVWzE1a6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TgKQ-ODqFI"
      },
      "source": [
        "## Data Provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-qBf7d7RDqFK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_dict = {\n",
        "    'ETTh1': Dataset_Custom_,#Dataset_ETT_hour,\n",
        "    'ETTm1': Dataset_Custom_,\n",
        "    'traffic': Dataset_Custom,\n",
        "    'electricity': Dataset_Custom_,\n",
        "    'exchange': Dataset_Custom_,\n",
        "    'weather': Dataset_Custom_,\n",
        "    'covid': Dataset_Covid,\n",
        "    'ECG': Dataset_Custom_,\n",
        "    'metr': Dataset_Custom_,\n",
        "}\n",
        "\n",
        "\n",
        "def data_provider(args, flag):\n",
        "    Data = data_dict[args.data]\n",
        "    timeenc = 0 if args.embed != 'timeF' else 1\n",
        "    train_only = args.train_only\n",
        "\n",
        "    if flag == 'test':\n",
        "        shuffle_flag = False\n",
        "        drop_last = True\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "    elif flag == 'pred':\n",
        "        shuffle_flag = False\n",
        "        drop_last = False\n",
        "        batch_size = 1\n",
        "        freq = args.freq\n",
        "        Data = Dataset_Pred\n",
        "    else:\n",
        "        shuffle_flag = True\n",
        "        drop_last = True\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "\n",
        "    data_set = Data(\n",
        "        root_path=args.root_path,\n",
        "        data_path=args.data_path,\n",
        "        flag=flag,\n",
        "        size=[args.seq_len, args.label_len, args.pred_len],\n",
        "        features=args.features,\n",
        "        target=args.target,\n",
        "        timeenc=timeenc,\n",
        "        freq=freq,\n",
        "        train_only=train_only\n",
        "    )\n",
        "    print(flag, len(data_set))\n",
        "    data_loader = DataLoader(\n",
        "        data_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle_flag,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last)\n",
        "    return data_set, data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5WCbD6DqFK"
      },
      "source": [
        "## Exp Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nlYUTVDKDqFL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Exp_Basic(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = self._acquire_device()\n",
        "        self.model = self._build_model().to(self.device)\n",
        "\n",
        "    def _build_model(self):\n",
        "        raise NotImplementedError\n",
        "        return None\n",
        "\n",
        "    def _acquire_device(self):\n",
        "        if self.args.use_gpu:\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
        "                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
        "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
        "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "            print('Use CPU')\n",
        "        return device\n",
        "\n",
        "    def _get_data(self):\n",
        "        pass\n",
        "\n",
        "    def vali(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    def test(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd_r7-67DqFL"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KNSRziYzDqFM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
        "    if args.lradj == 'type1':\n",
        "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
        "    elif args.lradj == 'type2':\n",
        "        lr_adjust = {\n",
        "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
        "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
        "        }\n",
        "    elif args.lradj == '3':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '4':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '5':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '6':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "class StandardScaler():\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "def visual(true, preds=None, name='./pic/test.pdf'):\n",
        "    \"\"\"\n",
        "    Results visualization\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
        "    if preds is not None:\n",
        "        plt.plot(preds, label='Prediction', linewidth=2)\n",
        "    plt.legend()\n",
        "    plt.savefig(name, bbox_inches='tight')\n",
        "\n",
        "def test_params_flop(model,x_shape):\n",
        "    \"\"\"\n",
        "    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()\n",
        "    \"\"\"\n",
        "    model_params = 0\n",
        "    for parameter in model.parameters():\n",
        "        model_params += parameter.numel()\n",
        "        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))\n",
        "    from ptflops import get_model_complexity_info\n",
        "    with torch.cuda.device(0):\n",
        "        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)\n",
        "        # print('Flops:' + flops)\n",
        "        # print('Params:' + params)\n",
        "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries import offsets\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "\n",
        "\n",
        "class TimeFeature:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"()\"\n",
        "\n",
        "\n",
        "class SecondOfMinute(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.second / 59.0 - 0.5\n",
        "\n",
        "\n",
        "class MinuteOfHour(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.minute / 59.0 - 0.5\n",
        "\n",
        "\n",
        "class HourOfDay(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.hour / 23.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfWeek(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.dayofweek / 6.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfMonth(TimeFeature):\n",
        "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.day - 1) / 30.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfYear(TimeFeature):\n",
        "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
        "\n",
        "\n",
        "class MonthOfYear(TimeFeature):\n",
        "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.month - 1) / 11.0 - 0.5\n",
        "\n",
        "\n",
        "class WeekOfYear(TimeFeature):\n",
        "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
        "\n",
        "\n",
        "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
        "    \"\"\"\n",
        "    Returns a list of time features that will be appropriate for the given frequency string.\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_str\n",
        "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
        "    \"\"\"\n",
        "\n",
        "    features_by_offsets = {\n",
        "        offsets.YearEnd: [],\n",
        "        offsets.QuarterEnd: [MonthOfYear],\n",
        "        offsets.MonthEnd: [MonthOfYear],\n",
        "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
        "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Minute: [\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "        offsets.Second: [\n",
        "            SecondOfMinute,\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    offset = to_offset(freq_str)\n",
        "\n",
        "    for offset_type, feature_classes in features_by_offsets.items():\n",
        "        if isinstance(offset, offset_type):\n",
        "            return [cls() for cls in feature_classes]\n",
        "\n",
        "    supported_freq_msg = f\"\"\"\n",
        "    Unsupported frequency {freq_str}\n",
        "    The following frequencies are supported:\n",
        "        Y   - yearly\n",
        "            alias: A\n",
        "        M   - monthly\n",
        "        W   - weekly\n",
        "        D   - daily\n",
        "        B   - business days\n",
        "        H   - hourly\n",
        "        T   - minutely\n",
        "            alias: min\n",
        "        S   - secondly\n",
        "    \"\"\"\n",
        "    raise RuntimeError(supported_freq_msg)\n",
        "\n",
        "\n",
        "def time_features(dates, freq='h'):\n",
        "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmT5VwnXDqFM"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IGHvs_TKDqFN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def RSE(pred, true):\n",
        "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\n",
        "\n",
        "\n",
        "def CORR(pred, true):\n",
        "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
        "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
        "    d += 1e-12\n",
        "    return 0.01*(u / d).mean(-1)\n",
        "\n",
        "\n",
        "def MAE(pred, true):\n",
        "    return np.mean(np.abs(pred - true))\n",
        "\n",
        "\n",
        "def MSE(pred, true):\n",
        "    return np.mean((pred - true) ** 2)\n",
        "\n",
        "\n",
        "def RMSE(pred, true):\n",
        "    return np.sqrt(MSE(pred, true))\n",
        "\n",
        "\n",
        "def MAPE(pred, true):\n",
        "    return np.mean(np.abs((pred - true) / true))\n",
        "\n",
        "\n",
        "def MSPE(pred, true):\n",
        "    return np.mean(np.square((pred - true) / true))\n",
        "\n",
        "\n",
        "def metric(pred, true):\n",
        "    mae = MAE(pred, true)\n",
        "    mse = MSE(pred, true)\n",
        "    rmse = RMSE(pred, true)\n",
        "    mape = MAPE(pred, true)\n",
        "    mspe = MSPE(pred, true)\n",
        "    rse = RSE(pred, true)\n",
        "    corr = CORR(pred, true)\n",
        "\n",
        "    return mae, mse, rmse, mape, mspe, rse, corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie1RzysrDqFN"
      },
      "source": [
        "## Exp Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fAR0bnNsDqFO"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Exp_Main(Exp_Basic):\n",
        "    def __init__(self, args):\n",
        "        super(Exp_Main, self).__init__(args)\n",
        "\n",
        "    def _build_model(self):\n",
        "        model_dict = {\n",
        "            'FreLinear': FreTS\n",
        "        }\n",
        "        model = model_dict[self.args.model].Model(self.args).float()\n",
        "\n",
        "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
        "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
        "        return model\n",
        "\n",
        "    def _get_data(self, flag):\n",
        "        data_set, data_loader = data_provider(self.args, flag)\n",
        "        return data_set, data_loader\n",
        "\n",
        "    def _select_optimizer(self):\n",
        "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
        "        return model_optim\n",
        "\n",
        "    def _select_criterion(self):\n",
        "        criterion = nn.MSELoss()\n",
        "        return criterion\n",
        "\n",
        "    def vali(self, vali_data, vali_loader, criterion):\n",
        "        total_loss = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
        "                batch_x = batch_x.float().to(self.device)\n",
        "                batch_y = batch_y.float()\n",
        "\n",
        "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
        "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
        "\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
        "                # encoder - decoder\n",
        "                if self.args.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                        else:\n",
        "                            if self.args.output_attention:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                            else:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                else:\n",
        "                    if 'Linear' in self.args.model:\n",
        "                        outputs = self.model(batch_x)\n",
        "                    else:\n",
        "                        if self.args.output_attention:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                        else:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                f_dim = -1 if self.args.features == 'MS' else 0\n",
        "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
        "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
        "\n",
        "                pred = outputs.detach().cpu()\n",
        "                true = batch_y.detach().cpu()\n",
        "\n",
        "                loss = criterion(pred, true)\n",
        "\n",
        "                total_loss.append(loss)\n",
        "        total_loss = np.average(total_loss)\n",
        "        self.model.train()\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, setting):\n",
        "        train_data, train_loader = self._get_data(flag='train')\n",
        "        if not self.args.train_only:\n",
        "            vali_data, vali_loader = self._get_data(flag='val')\n",
        "            test_data, test_loader = self._get_data(flag='test')\n",
        "\n",
        "        path = os.path.join(self.args.checkpoints, setting)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        time_now = time.time()\n",
        "\n",
        "        train_steps = len(train_loader)\n",
        "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
        "\n",
        "        model_optim = self._select_optimizer()\n",
        "        criterion = self._select_criterion()\n",
        "\n",
        "        total_params = 0\n",
        "        for name, parameter in self.model.named_parameters():\n",
        "            if not parameter.requires_grad: continue\n",
        "            param = parameter.numel()\n",
        "            total_params += param\n",
        "        print(f\"Total Trainable Params: {total_params}\")\n",
        "\n",
        "        if self.args.use_amp:\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        for epoch in range(self.args.train_epochs):\n",
        "            iter_count = 0\n",
        "            train_loss = []\n",
        "\n",
        "            self.model.train()\n",
        "            epoch_time = time.time()\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
        "                iter_count += 1\n",
        "                model_optim.zero_grad()\n",
        "                batch_x = batch_x.float().to(self.device)\n",
        "                batch_y = batch_y.float().to(self.device)\n",
        "\n",
        "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
        "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
        "\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
        "\n",
        "                # encoder - decoder\n",
        "                if self.args.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                        else:\n",
        "                            if self.args.output_attention:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                            else:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "\n",
        "                        f_dim = -1 if self.args.features == 'MS' else 0\n",
        "                        outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
        "                        batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "                        train_loss.append(loss.item())\n",
        "                else:\n",
        "                    if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                    else:\n",
        "                        if self.args.output_attention:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                        else:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, batch_y)\n",
        "                    # print(outputs.shape,batch_y.shape)\n",
        "                    f_dim = -1 if self.args.features == 'MS' else 0\n",
        "                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
        "                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    train_loss.append(loss.item())\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
        "                    speed = (time.time() - time_now) / iter_count\n",
        "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                if self.args.use_amp:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(model_optim)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    model_optim.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "            train_loss = np.average(train_loss)\n",
        "            if not self.args.train_only:\n",
        "                vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
        "                test_loss = self.vali(test_data, test_loader, criterion)\n",
        "\n",
        "                print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
        "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
        "                early_stopping(vali_loss, self.model, path)\n",
        "            else:\n",
        "                print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f}\".format(\n",
        "                    epoch + 1, train_steps, train_loss))\n",
        "                early_stopping(train_loss, self.model, path)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "            adjust_learning_rate(model_optim, epoch + 1, self.args)\n",
        "\n",
        "        best_model_path = path + '/' + 'checkpoint.pth'\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def test(self, setting, test=0):\n",
        "        test_data, test_loader = self._get_data(flag='test')\n",
        "\n",
        "        if test:\n",
        "            print('loading model')\n",
        "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
        "\n",
        "        preds = []\n",
        "        trues = []\n",
        "        inputx = []\n",
        "        folder_path = './test_results/' + setting + '/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
        "                batch_x = batch_x.float().to(self.device)\n",
        "                batch_y = batch_y.float().to(self.device)\n",
        "\n",
        "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
        "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
        "\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
        "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
        "                # encoder - decoder\n",
        "                if self.args.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                        else:\n",
        "                            if self.args.output_attention:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                            else:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                else:\n",
        "                    if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                    else:\n",
        "                        if self.args.output_attention:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "\n",
        "                        else:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "\n",
        "                f_dim = -1 if self.args.features == 'MS' else 0\n",
        "                # print(outputs.shape,batch_y.shape)\n",
        "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
        "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
        "                outputs = outputs.detach().cpu().numpy()\n",
        "                batch_y = batch_y.detach().cpu().numpy()\n",
        "\n",
        "                pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
        "                true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
        "\n",
        "                preds.append(pred)\n",
        "                trues.append(true)\n",
        "                inputx.append(batch_x.detach().cpu().numpy())\n",
        "                if i % 20 == 0:\n",
        "                    input = batch_x.detach().cpu().numpy()\n",
        "                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n",
        "                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n",
        "                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
        "\n",
        "        if self.args.test_flop:\n",
        "            test_params_flop((batch_x.shape[1],batch_x.shape[2]))\n",
        "            exit()\n",
        "        preds = np.array(preds)\n",
        "        trues = np.array(trues)\n",
        "        inputx = np.array(inputx)\n",
        "\n",
        "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
        "        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
        "\n",
        "        # result save\n",
        "        folder_path = './results/' + setting + '/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
        "        print('mse:{}, mae:{}, rmse:{}'.format(mse, mae, rmse))\n",
        "        f = open(\"result.txt\", 'a')\n",
        "        f.write(setting + \"  \\n\")\n",
        "        f.write('mse:{}, mae:{}, rse:{}, rmse:{}'.format(mse, mae, rse, rmse))\n",
        "        f.write('\\n')\n",
        "        f.write('\\n')\n",
        "        f.close()\n",
        "\n",
        "        # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n",
        "        np.save(folder_path + 'pred.npy', preds)\n",
        "        # np.save(folder_path + 'true.npy', trues)\n",
        "        # np.save(folder_path + 'x.npy', inputx)\n",
        "        return\n",
        "\n",
        "    def predict(self, setting, load=False):\n",
        "        pred_data, pred_loader = self._get_data(flag='pred')\n",
        "\n",
        "        if load:\n",
        "            path = os.path.join(self.args.checkpoints, setting)\n",
        "            best_model_path = path + '/' + 'checkpoint.pth'\n",
        "            self.model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(pred_loader):\n",
        "                batch_x = batch_x.float().to(self.device)\n",
        "                batch_y = batch_y.float()\n",
        "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
        "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
        "\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[2]]).float().to(batch_y.device)\n",
        "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
        "                # encoder - decoder\n",
        "                if self.args.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        if 'Linear' in self.args.model:\n",
        "                            outputs = self.model(batch_x)\n",
        "                        else:\n",
        "                            if self.args.output_attention:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                            else:\n",
        "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                else:\n",
        "                    if 'Linear' in self.args.model:\n",
        "                        outputs = self.model(batch_x)\n",
        "                    else:\n",
        "                        if self.args.output_attention:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
        "                        else:\n",
        "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
        "                pred = outputs.detach().cpu().numpy()  # .squeeze()\n",
        "                preds.append(pred)\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "        if (pred_data.scale):\n",
        "            preds = pred_data.inverse_transform(preds)\n",
        "\n",
        "        # result save\n",
        "        folder_path = './results/' + setting + '/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        np.save(folder_path + 'real_prediction.npy', preds)\n",
        "        pd.DataFrame(np.append(np.transpose([pred_data.future_dates]), preds[0], axis=1), columns=pred_data.cols).to_csv(folder_path + 'real_prediction.csv', index=False)\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODWlQ14kDqFP"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UA2JK9bSDqFP"
      },
      "outputs": [],
      "source": [
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lzq0Z0WdDqFP"
      },
      "outputs": [],
      "source": [
        "class args:\n",
        "    is_training=1\n",
        "    train_only=False\n",
        "    model_id='ExchangeTrial96'\n",
        "    model='FreLinear'\n",
        "    data='exchange'\n",
        "    root_path='./dataset/'\n",
        "    data_path='exchange_rate.csv'\n",
        "    channel_independence=0\n",
        "    features='M'\n",
        "    target='OT'\n",
        "    freq='h'\n",
        "    checkpoints='./checkpoints/'\n",
        "    seq_len=96\n",
        "    label_len=48\n",
        "    pred_len=96\n",
        "    individual=False\n",
        "    embed_type=0\n",
        "    enc_in=7\n",
        "    dec_in=7\n",
        "    c_out=7\n",
        "    d_model=512\n",
        "    n_heads=8\n",
        "    e_layers=2\n",
        "    d_layers=1\n",
        "    d_ff=2048\n",
        "    moving_avg=25\n",
        "    factor=1\n",
        "    distil=True\n",
        "    dropout=0.05\n",
        "    embed='timeF'\n",
        "    activation='gelu'\n",
        "    output_attention=False\n",
        "    do_predict=False\n",
        "    num_workers=0\n",
        "    itr=1\n",
        "    train_epochs=500\n",
        "    batch_size=50\n",
        "    patience=3\n",
        "    learning_rate=0.0001\n",
        "    des='Exp'\n",
        "    loss='mse'\n",
        "    lradj='type1'\n",
        "    use_amp=False\n",
        "    use_gpu=True\n",
        "    gpu=0\n",
        "    use_multi_gpu=False\n",
        "    devices='0,1,2'\n",
        "    test_flop=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMEIS1LbDqFP",
        "outputId": "93d19bab-11a4-4cf5-fdd5-545590804a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "<class '__main__.args'>\n"
          ]
        }
      ],
      "source": [
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.dvices = args.devices.replace(' ', '')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "\n",
        "print('Args in experiment:')\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-k4Sk8oDqFQ",
        "outputId": "e345c47d-8903-4a56-ff16-fbec6e68127a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : ExchangeTrial96_FreLinear_exchange_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 5120\n",
            "val 1424\n",
            "test 663\n",
            "Total Trainable Params: 3236832\n",
            "\titers: 100, epoch: 1 | loss: 0.0065862\n",
            "\tspeed: 0.0151s/iter; left time: 768.9238s\n",
            "Epoch: 1 cost time: 1.5426125526428223\n",
            "Epoch: 1, Steps: 102 | Train Loss: 0.0157684 Vali Loss: 0.0069405 Test Loss: 0.0050341\n",
            "Validation loss decreased (inf --> 0.006940).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.0055324\n",
            "\tspeed: 0.0187s/iter; left time: 948.7056s\n",
            "Epoch: 2 cost time: 1.5090439319610596\n",
            "Epoch: 2, Steps: 102 | Train Loss: 0.0073909 Vali Loss: 0.0070563 Test Loss: 0.0048450\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.0070791\n",
            "\tspeed: 0.0181s/iter; left time: 916.8483s\n",
            "Epoch: 3 cost time: 1.5088677406311035\n",
            "Epoch: 3, Steps: 102 | Train Loss: 0.0067811 Vali Loss: 0.0059217 Test Loss: 0.0046470\n",
            "Validation loss decreased (0.006940 --> 0.005922).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0055328\n",
            "\tspeed: 0.0179s/iter; left time: 905.4593s\n",
            "Epoch: 4 cost time: 1.4790863990783691\n",
            "Epoch: 4, Steps: 102 | Train Loss: 0.0065945 Vali Loss: 0.0069180 Test Loss: 0.0046109\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.0066680\n",
            "\tspeed: 0.0175s/iter; left time: 881.9875s\n",
            "Epoch: 5 cost time: 1.4765541553497314\n",
            "Epoch: 5, Steps: 102 | Train Loss: 0.0065048 Vali Loss: 0.0063410 Test Loss: 0.0044592\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.0046245\n",
            "\tspeed: 0.0175s/iter; left time: 879.8772s\n",
            "Epoch: 6 cost time: 1.4787909984588623\n",
            "Epoch: 6, Steps: 102 | Train Loss: 0.0064581 Vali Loss: 0.0064557 Test Loss: 0.0044692\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : ExchangeTrial96_FreLinear_exchange_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 663\n",
            "mse:0.004646997433155775, mae:0.05041668191552162, rmse:0.06816888600587845\n"
          ]
        }
      ],
      "source": [
        "Exp = Exp_Main\n",
        "\n",
        "if args.is_training:\n",
        "    for ii in range(args.itr):\n",
        "        # setting record of experiments\n",
        "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
        "            args.model_id,\n",
        "            args.model,\n",
        "            args.data,\n",
        "            args.features,\n",
        "            args.seq_len,\n",
        "            args.label_len,\n",
        "            args.pred_len,\n",
        "            args.d_model,\n",
        "            args.n_heads,\n",
        "            args.e_layers,\n",
        "            args.d_layers,\n",
        "            args.d_ff,\n",
        "            args.factor,\n",
        "            args.embed,\n",
        "            args.distil,\n",
        "            args.des, ii)\n",
        "\n",
        "        exp = Exp(args)  # set experiments\n",
        "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "        exp.train(setting)\n",
        "\n",
        "        if not args.train_only:\n",
        "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "            exp.test(setting)\n",
        "\n",
        "        if args.do_predict:\n",
        "            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "            exp.predict(setting, True)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "else:\n",
        "    ii = 0\n",
        "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
        "                                                                                                  args.model,\n",
        "                                                                                                  args.data,\n",
        "                                                                                                  args.features,\n",
        "                                                                                                  args.seq_len,\n",
        "                                                                                                  args.label_len,\n",
        "                                                                                                  args.pred_len,\n",
        "                                                                                                  args.d_model,\n",
        "                                                                                                  args.n_heads,\n",
        "                                                                                                  args.e_layers,\n",
        "                                                                                                  args.d_layers,\n",
        "                                                                                                  args.d_ff,\n",
        "                                                                                                  args.factor,\n",
        "                                                                                                  args.embed,\n",
        "                                                                                                  args.distil,\n",
        "                                                                                                  args.des, ii)\n",
        "\n",
        "    exp = Exp(args)  # set experiments\n",
        "\n",
        "    if args.do_predict:\n",
        "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "        exp.predict(setting, True)\n",
        "    else:\n",
        "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "        exp.test(setting, test=1)\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}